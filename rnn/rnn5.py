import tensorflow as tf
import numpy as np

tf.reset_default_graph()

# Create input data
X = np.random.randn(2, 10, 8)

# The second example is of length 6 
X[1,6,:] = 0
X_lengths = [10, 6]

cell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=True)
cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)
cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell] * 4, state_is_tuple=True)

outputs, last_states = tf.nn.dynamic_rnn(
    cell=cell,
    dtype=tf.float64,
    sequence_length=X_lengths,
    inputs=X)

result = tf.contrib.learn.run_n(
    {"outputs": outputs, "last_states": last_states},
    n=1,
    feed_dict=None)


print(result[0]["outputs"].shape)
print(result[0]["outputs"])
assert result[0]["outputs"].shape == (2, 10, 64)

# Outputs for the second example past past length 6 should be 0
assert (result[0]["outputs"][1,7,:] == np.zeros(cell.output_size)).all()

print(result[0]["last_states"][0].h.shape)
print(result[0]["last_states"][0].h)
'''
(2, 10, 64)
[[[-0.         -0.          0.         ..., -0.          0.          0.00049422]
  [-0.00506018 -0.          0.         ..., -0.00153762  0.00940529
   -0.00298744]
  [-0.         -0.          0.00207412 ..., -0.          0.01325014
   -0.00184734]
  ..., 
  [-0.         -0.          0.02858326 ..., -0.          0.0380286   0.        ]
  [ 0.         -0.          0.         ..., -0.          0.         -0.        ]
  [-0.         -0.          0.         ..., -0.          0.06674728
    0.00991639]]

 [[ 0.          0.         -0.00085455 ..., -0.         -0.00024531 -0.        ]
  [-0.         -0.          0.         ..., -0.          0.          0.0001503 ]
  [-0.         -0.00071203  0.         ...,  0.          0.00174803
   -0.01482452]
  ..., 
  [ 0.          0.          0.         ...,  0.          0.          0.        ]
  [ 0.          0.          0.         ...,  0.          0.          0.        ]
  [ 0.          0.          0.         ...,  0.          0.          0.        ]]]
(2, 64)
[[ 0.01297067  0.22907527  0.05139425  0.29435922  0.04605551  0.00754729
  -0.02567375 -0.05589294 -0.04368968 -0.13239893  0.12788581 -0.0152978
  -0.12564234  0.16885918  0.08673705  0.11510634  0.25851612 -0.15981472
  -0.24101715 -0.02910687 -0.05704777 -0.0625448   0.09622846  0.18139745
   0.08995436  0.10204869  0.07207662 -0.07396717 -0.0219263   0.01405567
   0.17110585  0.01182969 -0.0498644   0.07279543  0.10803916  0.00569787
  -0.15314169  0.27198361 -0.14346144  0.15778599  0.0366264   0.24657414
  -0.09828316 -0.02579282  0.20024947 -0.08989078  0.08683931  0.32990483
  -0.34627406  0.19013834  0.16285538  0.01712701 -0.24724586  0.05107909
   0.09261759  0.12964262 -0.06928847  0.05859123 -0.0393647  -0.05223164
  -0.12332454  0.22110182 -0.00410163 -0.23566517]
 [-0.04395366 -0.15176906  0.22472317  0.07080675  0.07240523  0.02736522
   0.16006891 -0.01704436 -0.09491751 -0.0216078  -0.00414679  0.03489594
  -0.09223871  0.07047478 -0.13670871 -0.01044186 -0.07979238  0.12859565
   0.05801918 -0.00884017  0.02661021 -0.00717293 -0.00491826 -0.11241522
   0.04202246  0.10015328  0.00875717 -0.04298044 -0.00603086  0.06858007
  -0.11210348  0.03872368 -0.01849077 -0.04118327  0.02222942 -0.0317735
   0.04579761 -0.14684348  0.07929213 -0.05711392 -0.05404523  0.01521693
   0.02289623 -0.0979958  -0.06607561 -0.08151365 -0.08028146 -0.09242943
   0.03828567  0.0088612  -0.13851357  0.02198162  0.02416086  0.07779938
  -0.03962825 -0.08398527 -0.0602596  -0.17855306 -0.01223452 -0.09552392
   0.01105561 -0.00628815 -0.11230546  0.01049755]]
   '''